{"cells":[{"cell_type":"markdown","id":"42e8a289","metadata":{"id":"42e8a289"},"source":["# Nullspace BERT Demonstration"]},{"cell_type":"markdown","id":"58d3e044","metadata":{"id":"58d3e044"},"source":["##### Install Requirements"]},{"cell_type":"code","execution_count":1,"id":"0fd7dde6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fd7dde6","executionInfo":{"status":"ok","timestamp":1648776924288,"user_tz":240,"elapsed":1864,"user":{"displayName":"jy chen","userId":"04298825870823504576"}},"outputId":"a923ddbe-d74b-41c5-ba1d-ccdfb9da6f48"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","id":"1bc2bc67","metadata":{"id":"1bc2bc67"},"source":["---"]},{"cell_type":"code","execution_count":null,"id":"8190f8f2","metadata":{"id":"8190f8f2"},"outputs":[],"source":["import numpy as np\n","from tqdm import tqdm\n","import transformers\n","from transformers import (\n","    BertTokenizer,\n","    BertForMaskedLM\n",")\n","import pandas as pd\n","import torch\n","from torch.nn import functional as F\n","from transformers import logging\n","\n","logging.set_verbosity_error()"]},{"cell_type":"markdown","id":"fffd356e","metadata":{"id":"fffd356e"},"source":["# BERT Transformer"]},{"cell_type":"code","execution_count":null,"id":"0b18fbaa","metadata":{"id":"0b18fbaa"},"outputs":[],"source":["bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True)"]},{"cell_type":"markdown","id":"f1a6d155","metadata":{"id":"f1a6d155"},"source":["![title](images/bert_architecture.png)"]},{"cell_type":"markdown","id":"add7e792","metadata":{"id":"add7e792"},"source":["Source: [**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova](https://arxiv.org/abs/1810.04805)"]},{"cell_type":"code","execution_count":null,"id":"0ac0bc7e","metadata":{"id":"0ac0bc7e"},"outputs":[],"source":["BERT_BIAS_LAYER = 12\n","\n","def chunker(input_list, chunk_size):\n","    \"\"\"split sequence into chunks\"\"\"\n","    for i in range(0, len(input_list), chunk_size):\n","        yield input_list[i:i + chunk_size]\n","        \n","def get_embeddings(input_sequences, model, tokenizer):\n","    \"\"\"extract hidden state from nth attention layer in encoder as specified by BERT_BIAS_LAYER\"\"\"\n","    tokenized_input = bert_tokenizer.batch_encode_plus(input_sequences, return_tensors = \"pt\", padding=True, truncation=False)\n","    embeddings = bert_mlm(**tokenized_input, output_hidden_states=True).hidden_states[BERT_BIAS_LAYER]\n","    return embeddings.detach().numpy(), tokenized_input[\"input_ids\"].detach().numpy()\n","\n","def extract_token_embeddings(embeddings, input_ids):\n","    \"\"\"filter special token embeddings\"\"\"\n","    extracted_embeddings = []\n","    for idx in range(embeddings.shape[0]):\n","        if 0 in input_ids[idx]: # if input contains padding\n","            eos_idx = list(input_ids[idx]).index(0) - 1\n","        else:\n","            eos_idx = list(input_ids[idx]).index(102)\n","        extracted_embeddings.append(embeddings[idx][1:eos_idx].mean(axis=0))\n","    return np.array(extracted_embeddings)"]},{"cell_type":"markdown","id":"60c31abe","metadata":{"id":"60c31abe"},"source":["# BERT Vector Generation"]},{"cell_type":"markdown","id":"9de9527c","metadata":{"id":"9de9527c"},"source":["![title](images/bert_layers_gender_bias.png)"]},{"cell_type":"markdown","id":"aee43de6","metadata":{"id":"aee43de6"},"source":["Source: [**Investigating Gender Bias in BERT**: Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria](https://arxiv.org/abs/2009.05021)"]},{"cell_type":"code","execution_count":null,"id":"593352f2","metadata":{"id":"593352f2","outputId":"81f22407-304b-4c57-dc02-b3172926a75d"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'dict'>\n","400000\n"]}],"source":["import gensim\n","from gensim import downloader\n","from gensim.models import KeyedVectors\n","# word2vec = gensim.downloader.load('glove-wiki-gigaword-300')\n","# word2vec.save('vectors.kv')\n","word2vec = KeyedVectors.load('vectors.kv')\n","print(type(word2vec.key_to_index))\n","print(len(word2vec.key_to_index))\n","VOCABULARY = list(word2vec.key_to_index.keys())"]},{"cell_type":"code","execution_count":null,"id":"1ffad7dd","metadata":{"id":"1ffad7dd","outputId":"8c9c0650-c143-4510-d0ba-bad7dd730f72"},"outputs":[{"name":"stderr","output_type":"stream","text":["9it [05:31, 36.57s/it]"]}],"source":["sub_vocab = VOCABULARY[0:10000]\n","bert_vocab_embedding_list = np.empty((0, 768))\n","for chunk in tqdm(chunker(sub_vocab, 1000)):\n","    embeddings, input_ids = get_embeddings(chunk, bert_mlm, bert_tokenizer)\n","    embeddings = extract_token_embeddings(embeddings, input_ids)\n","    bert_vocab_embedding_list = np.concatenate((bert_vocab_embedding_list, embeddings), axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e5313ed0","metadata":{"id":"e5313ed0"},"outputs":[],"source":["sub_vocab = VOCABULARY[10000:20000]\n","for chunk in tqdm(chunker(sub_vocab, 1000)):\n","    embeddings, input_ids = get_embeddings(chunk, bert_mlm, bert_tokenizer)\n","    embeddings = extract_token_embeddings(embeddings, input_ids)\n","    bert_vocab_embedding_list = np.concatenate((bert_vocab_embedding_list, embeddings), axis=0)"]},{"cell_type":"code","execution_count":null,"id":"53c31aea","metadata":{"id":"53c31aea"},"outputs":[],"source":["sub_vocab = VOCABULARY[20000:30000]\n","for chunk in tqdm(chunker(sub_vocab, 1000)):\n","    embeddings, input_ids = get_embeddings(chunk, bert_mlm, bert_tokenizer)\n","    embeddings = extract_token_embeddings(embeddings, input_ids)\n","    bert_vocab_embedding_list = np.concatenate((bert_vocab_embedding_list, embeddings), axis=0)"]},{"cell_type":"code","execution_count":null,"id":"84f15cbc","metadata":{"id":"84f15cbc"},"outputs":[],"source":["sub_vocab = VOCABULARY[30000:40000]\n","for chunk in tqdm(chunker(sub_vocab, 1000)):\n","    embeddings, input_ids = get_embeddings(chunk, bert_mlm, bert_tokenizer)\n","    embeddings = extract_token_embeddings(embeddings, input_ids)\n","    bert_vocab_embedding_list = np.concatenate((bert_vocab_embedding_list, embeddings), axis=0)"]},{"cell_type":"code","execution_count":null,"id":"57cc7d0f","metadata":{"id":"57cc7d0f"},"outputs":[],"source":["embedding_shape = bert_vocab_embedding_list.shape\n","with open(\"data/embeddings/BERTLM_ENCODER_LAYER_ONE/bert-base-uncased-embeddings.txt\", \"w\") as bert_file:\n","    bert_file.write(\"40000 768\")\n","    for word, embedding in zip(VOCABULARY, bert_vocab_embedding_list):\n","        bert_file.write(f\"{word} {' '.join(map(str, list(embedding)))}\\n\")"]},{"cell_type":"code","execution_count":null,"id":"f1df220b","metadata":{"id":"f1df220b","outputId":"21c3416d-b15a-417e-c0cc-6c744f3f3b2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["pca explained variance ratio: [0.32702205 0.20407498 0.15900119 0.08174839 0.05758674 0.05069627\n"," 0.04543993 0.03472263 0.02933716 0.01037071] \n","\n","TOP 100 MALE SENSITIVE TOKENS \n"," ('man', 'john', 'mano', 'he', 'boy', 'guy', 'mancuso', 'son', 'his', 'mancini', 'manhunt', 'him', 'manger', 'boynton', 'manx', 'manmohan', 'hebei', 'heinous', 'hisham', 'boyce', 'manley', 'himself', 'heisman', 'hebron', 'jackman', 'hester', 'manu', 'mandating', 'heim', 'sons', 'kidman', 'mandel', 'mangled', 'heston', 'hebert', '10-man', 'man-made', 'helton', 'brothers', 'heist', 'dutchman', 'father', 'hitman', 'brother', 'hoo', 'mike', 'menlo', 'boyish', 'boyz', 'manhood', 'fatherland', 'bradman', 'beckman', 'handyman', 'himachal', 'dude', 'mendocino', 'hillman', 'jason', 'johndroe', 'paceman', 'mr', 'scotsman', 'boys', 'redman', 'linesman', 'hectic', 'mrt', 'mandelson', 'charles', 'rodman', 'manure', 'hezb', 'edgardo', 'cashman', 'sonoma', 'horan', 'handsome', 'boyhood', 'ackerman', 'pittman', 'siro', 'horrendous', 'timo', 'men', 'cocky', 'walter', 'guyanese', 'sonja', 'heh', 'hopman', 'rothman', 'james', 'robert', 'matthew', 'usman', 'hoon', 'himmler', 'irishman', 'mr.')\n","\n","\n","TOP 100 FEMALE SENSITIVE TOKENS \n"," ('mary', 'she', 'woman', 'women', 'feminism', 'feminist', 'sheryl', 'actresses', 'lesbian', 'herself', 'goddess', 'aunt', 'actress', 'mother', 'grandmother', 'female', 'shelved', 'shetty', 'her', 'heroine', 'galina', 'nursing', 'empress', 'maternity', 'wnba', 'housekeeper', 'mothers', 'sisters', 'daughters', 'sorority', 'hostess', 'countess', 'edith', 'madam', 'gal', 'mrs', 'pregnant', 'queen', 'wta', 'nurses', 'girl', 'females', 'prostitute', 'granddaughter', 'daughter', 'ladies', 'sister', 'sheehan', 'baroness', 'convent', 'witches', 'prostitutes', 'girls', 'niece', 'duchess', 'bitch', 'nuns', 'childbirth', 'lady', 'hers', 'sheh', 'mildred', 'abby', 'chairperson', 'nurse', 'sheik', 'katie', 'maggie', 'louise', 'marjorie', 'softball', 'feminine', 'waitress', 'madame', 'schoolgirl', 'roxy', 'mistress', 'dowager', 'stepmother', 'durga', 'hertha', 'shee', 'princess', 'elisabeth', 'luisa', 'cheerleading', 'netball', 'agnes', 'barbara', 'hernia', 'jill', 'grandma', 'widowed', 'feminists', 'diva', 'patty', 'irina', 'dinah', 'granny', 'josie')\n","\n","\n","TOP 100 NEUTRAL TOKENS \n"," ('traumatized', 'charmed', 'keynes', 'dy', 'bonner', 'offsetting', 'barrymore', 'marlene', 'buckinghamshire', 'seaboard', 'funneled', 'coria', 'dunham', 'delaney', 'moeller', 'abdelaziz', 'biochemistry', 'embodies', 'sizzling', 'kos', 'revitalization', 'impeccable', '57-year', 'baquba', 'cpr', 'cons', 'taker', 'chests', 'jagged', 'ecfa', 'rankin', 'utopia', 'swirl', 'vanishing', 'grooming', 'aruba', 'outbursts', 'jenson', '86th', 'forewings', 'gravy', 'radars', 'fern√°ndez', 'korda', 'replenish', 'clique', 'clegg', 'palmeiro', 'caches', 'vertices', 'toad', 'barricaded', 'hush', 'canons', 'disallowed', 'dazed', 'malignant', 'viewpoints', 'upholds', 'quash', 'rua', 'withstood', 'anderlecht', 'wehrmacht', 'guideline', 'deportations', 'almighty', 'orthopedic', 'peer-reviewed', '268', 'gonzaga', 'unequivocal', 'chivas', 'sochaux', 'reintroduced', 'megabytes', 'heighten', '5.75', 'neutralize', 'intruders', 'scud', 'payoffs', '271', 'mattresses', 'dwarfs', 'carmichael', 'phosphorus', 'secede', 'booms', 'feeble', '89th', 'labonte', 'loya', 'leary', 'evander', 'deficient', 'wallets', 'jurisprudence', 'sakic', 'restores')\n","\n","\n"]}],"source":["!python get_bias_sensitive_tokens.py"]},{"cell_type":"markdown","id":"387a19bf","metadata":{"id":"387a19bf"},"source":["# Null-Space Projection"]},{"cell_type":"code","execution_count":null,"id":"e1ca7e93","metadata":{"id":"e1ca7e93","outputId":"ee2deb6e-d35c-45c5-f8ba-35374428db52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 147; Dev size: 63; Test size: 90\n","iteration: 24, accuracy: 0.36507936507936506: 100%|‚ñà| 25/25 [00:08<00:00,  3.12i\n","Figure(600x500)\n","Figure(600x500)\n","V-measure-before (TSNE space): 0.778190793392485\n","V-measure-after (TSNE space): 0.0011550932483761207\n","V-measure-before (original space): 1.0\n","V-measure-after (original space): 0.0007205831499929152\n"]}],"source":["!python context_nullspace_projection.py"]},{"cell_type":"markdown","id":"f8a4ead3","metadata":{"id":"f8a4ead3"},"source":["![title](images/tsne_projections.png)"]},{"cell_type":"markdown","id":"08a8e4bf","metadata":{"id":"08a8e4bf"},"source":["# Transformer Encoder / Decoder Generation"]},{"cell_type":"code","execution_count":null,"id":"5a012d4a","metadata":{"id":"5a012d4a"},"outputs":[],"source":["NULL_PROJECTION = np.load(\"data/nullspace_vector.npy\")"]},{"cell_type":"code","execution_count":null,"id":"57e6ee6f","metadata":{"id":"57e6ee6f"},"outputs":[],"source":["def guard_vector(layer):\n","    \"\"\"Apply nullprojection to inputted vector\"\"\"\n","    return NULL_PROJECTION.dot(layer.T).T\n","\n","\n","def guard_embedding(hidden_state, tokenized_input):\n","    \"\"\"Apply the linear guarding function to hidden state\"\"\"\n","    input_ids_numpy = list(tokenized_input[\"input_ids\"].detach().numpy()[0])\n","    word_indexes = [input_ids_numpy.index(token_id) for token_id in input_ids_numpy if token_id not in [101, 103, 102, 0]]\n","    bias_layer_numpy =  hidden_state.detach().numpy()\n","    for idx in word_indexes:\n","        bias_layer_numpy[0][idx] = guard_vector(bias_layer_numpy[0][idx])\n","    return torch.Tensor(bias_layer_numpy)\n","\n","\n","def run_post_bias_encoder_layers(encoder_layers_list, previous_hidden_state):\n","    \"\"\"Manually run embeddings through attention blocks in encoder\"\"\"\n","    for attention_block in encoder_layers_list:\n","        previous_hidden_state = attention_block.forward(hidden_states=previous_hidden_state)[0]\n","    return previous_hidden_state\n","\n","\n","def get_next_word(logits, tokenizer, mask_index):\n","    \"\"\"Generate the next highest liklihood word given logits\"\"\"\n","    softmax = F.softmax(logits, dim = -1)\n","    mask_word = softmax[0, mask_index, :]\n","    top_word = torch.argmax(mask_word, dim=1)\n","    return tokenizer.decode(top_word)\n","\n","\n","def generate_next_word(input_sequence, model, tokenizer, guard_flag=False, biased_layer_index=1):\n","    # extracting modules from BERT LM\n","    bert_encoder_modules = list(bert_mlm.modules())[8:-5] # extract list of model components\n","    encoder_layers_list = [bert_encoder_modules[idx] for idx in range(19, 206, 17)] # extracting each encoder attention block\n","    bert_mlm_head = bert_encoder_modules[-1] # extracting BERT LM Head\n","\n","    # tokenize input sequence\n","    tokenized_input = tokenizer.encode_plus(input_sequence, return_tensors = \"pt\")\n","    mask_index = torch.where(tokenized_input[\"input_ids\"][0] == bert_tokenizer.mask_token_id)\n","\n","    # extracting encoding and feeding back into model\n","    hidden_state = model(**tokenized_input, output_hidden_states=True).hidden_states[biased_layer_index]\n","    \n","    # apply guarding function to hidden state\n","    hidden_state = guard_embedding(hidden_state, tokenized_input) if guard_flag else hidden_state\n","    \n","    # run guarded hidden state through remaining encoder layers\n","    encoder_output = run_post_bias_encoder_layers(encoder_layers_list, hidden_state)\n","    \n","    # pass encoder output into LM Head to generate logits\n","    output_logits = bert_mlm_head.forward(sequence_output=encoder_output)\n","\n","    # generate the highest likelihood word\n","    return get_next_word(output_logits, tokenizer, mask_index)\n","\n","\n","def complete_the_sentence(female_variant, male_variant):\n","    guarded_pred_female = generate_next_word(female_variant, bert_mlm, bert_tokenizer, guard_flag=True)\n","    unguarded_pred_female = generate_next_word(female_variant, bert_mlm, bert_tokenizer, guard_flag=False)\n","    guarded_pred_male = generate_next_word(male_variant, bert_mlm, bert_tokenizer, guard_flag=True)\n","    unguarded_pred_male = generate_next_word(male_variant, bert_mlm, bert_tokenizer, guard_flag=False)\n","\n","    guarded_female_sentence = female_variant.replace(\"[MASK]\", guarded_pred_female.upper())\n","    unguarded_female_sentence = female_variant.replace(\"[MASK]\", unguarded_pred_female.upper())\n","    guarded_male_sentence = male_variant.replace(\"[MASK]\", guarded_pred_male.upper())\n","    unguarded_male_sentence = male_variant.replace(\"[MASK]\", unguarded_pred_male.upper())\n","    output_string = (f\"unguarded outputs: \\n \\t{unguarded_female_sentence} \\n \\t{unguarded_male_sentence} \\n\\n\" +\n","                     f\"guarded outputs: \\n \\t{guarded_female_sentence} \\n \\t{guarded_male_sentence} \\n\")\n","    print(output_string)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"cff37cfa","metadata":{"id":"cff37cfa","outputId":"32a1a642-f35b-428c-8565-457bb3094d99"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","-------------- people thought <she/he> was very [MASK] during the meeting. -----------------\n","\n","unguarded outputs: \n"," \tpeople thought she was very BEAUTIFUL during the meeting. \n"," \tpeople thought he was very POPULAR during the meeting. \n","\n","guarded outputs: \n"," \tpeople thought she was very VERY during the meeting. \n"," \tpeople thought he was very VERY during the meeting. \n","\n"]}],"source":["print(\"\\n-------------- people thought <she/he> was very [MASK] during the meeting. -----------------\\n\")\n","complete_the_sentence(\"people thought she was very \" + bert_tokenizer.mask_token + \" during the meeting.\",\n","                      \"people thought he was very \" + bert_tokenizer.mask_token + \" during the meeting.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"name":"nullspace_bert_demonstration.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}